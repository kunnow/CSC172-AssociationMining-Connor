{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a279f26",
   "metadata": {},
   "source": [
    "# 02 - Exploratory Data Analysis & Transaction Transformation\n",
    "\n",
    "This notebook loads `data/processed/cleaned_phishing.csv`, performs EDA (class distribution, token frequencies, basket sizes), and converts cleaned text into a binary transaction matrix for Apriori. The transaction matrix is saved to `data/processed/transactions.pkl`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92904695",
   "metadata": {},
   "source": [
    "### Load Processed Data and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc0f21f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "IN = Path(\"../data/processed/cleaned_phishing.csv\")\n",
    "TRAN_PKL = Path(\"../data/processed/transactions.pkl\")\n",
    "\n",
    "df = pd.read_csv(IN)\n",
    "print(\"Loaded cleaned data:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42f3753",
   "metadata": {},
   "source": [
    "### EDA: class balance and basket size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a9bd1a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Class distribution (if label exists)\n",
    "if 'label' in df.columns:\n",
    "    sns.countplot(x='label', data=df)\n",
    "    plt.title('Label distribution (0=legit, 1=phish)')\n",
    "    plt.show()\n",
    "\n",
    "# Basket sizes (word counts)\n",
    "sns.histplot(df['word_count'], bins=50, kde=True)\n",
    "plt.title('Distribution of word counts (basket sizes)')\n",
    "plt.xlabel('Word count')\n",
    "plt.show()\n",
    "\n",
    "print(\"Basket size stats:\", df['word_count'].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90ab999",
   "metadata": {},
   "source": [
    "### Top Tokens (Phising Subset Option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f1ee64",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Option: analyze phishing-only tokens to focus ARM on phishing patterns\n",
    "phish_only = True  # set False to use entire corpus\n",
    "df_for_tokens = df[df['label']==1] if (phish_only and 'label' in df.columns) else df\n",
    "\n",
    "all_tokens = ' '.join(df_for_tokens['clean_text'].astype(str)).split()\n",
    "cnt = Counter(all_tokens)\n",
    "top20 = cnt.most_common(20)\n",
    "print(\"Top 20 tokens ({}):\".format('phishing' if phish_only else 'all'))\n",
    "for t,c in top20:\n",
    "    print(f\"{t}: {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24828b2",
   "metadata": {},
   "source": [
    "### Transaction Transformation (CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e3bba1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "\n",
    "# Vectorizer parameters â€” tune for memory / quality\n",
    "vectorizer = CountVectorizer(\n",
    "    binary=True,\n",
    "    min_df=0.002,      # token must appear in >=0.2% of docs; adjust lower if too few itemsets\n",
    "    max_features=1000  # cap features for runtime; increase if memory allows\n",
    ")\n",
    "\n",
    "# Fit on the *clean_text* of the data we want to mine (phish-only or all)\n",
    "corpus = df_for_tokens['clean_text'].astype(str).values\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "transaction_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "print(\"Transaction matrix shape:\", transaction_df.shape)\n",
    "\n",
    "# Save transactions (pickle)\n",
    "TRAN_PKL.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(TRAN_PKL, \"wb\") as f:\n",
    "    pickle.dump({'transaction_df': transaction_df, 'vectorizer_vocab': vectorizer.get_feature_names_out()}, f)\n",
    "print(\"Saved transactions to\", TRAN_PKL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eb4004",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- `min_df` controls token frequency threshold; lower it to include rarer tokens at the cost of more itemsets and runtime.\n",
    "- `max_features` limits vocabulary to keep Apriori tractable. If you have sufficient memory and want more detail, increase this value.\n",
    "- We saved a pickle with the transaction DataFrame and vocabulary for reproducibility."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
