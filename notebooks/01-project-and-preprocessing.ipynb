{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e32aa60e",
   "metadata": {},
   "source": [
    "# 01 - Project Overview & Data Preprocessing\n",
    "\n",
    "**Project title:** Discovering Behavioral Patterns in Phishing Emails Using Association Rule Mining\n",
    "\n",
    "**Problem statement (short):**\n",
    "Phishing emails exploit recurring tactics (urgency, impersonation, deceptive links). This project uses Association Rule Mining (Apriori + rule evaluation) on a cleaned, combined phishing email dataset to surface frequent co-occurring tokens and interpretable rules (support, confidence, lift) that characterize phishing behavior.\n",
    "\n",
    "This notebook loads the final dataset, performs text cleaning, and writes a cleaned CSV to `data/processed/cleaned_phishing.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd33a64",
   "metadata": {},
   "source": [
    "### Imports and Dataset Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09bdff27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script tqdm.exe is installed in 'c:\\Users\\jradl\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script nltk.exe is installed in 'c:\\Users\\jradl\\AppData\\Local\\Programs\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 1.8 MB/s eta 0:00:00\n",
      "Collecting click\n",
      "  Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "     -------------------------------------- 108.3/108.3 kB 3.2 MB/s eta 0:00:00\n",
      "Collecting joblib\n",
      "  Downloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "     -------------------------------------- 309.1/309.1 kB 2.4 MB/s eta 0:00:00\n",
      "Collecting regex>=2021.8.3\n",
      "  Downloading regex-2025.11.3-cp311-cp311-win_amd64.whl (277 kB)\n",
      "     -------------------------------------- 277.7/277.7 kB 1.4 MB/s eta 0:00:00\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\jradl\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.3.1 joblib-1.5.3 nltk-3.9.2 regex-2025.11.3 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Raw shape: (82486, 2)\n",
      "Columns: ['text_combined', 'label']\n",
      "Using text column: text_combined\n",
      "Using label column: label\n"
     ]
    }
   ],
   "source": [
    "%pip install nltk\n",
    "# Imports\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# NLTK resources (download once)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Paths\n",
    "RAW = Path(\"../data/raw/final/phishing_email.csv\")\n",
    "OUT = Path(\"../data/processed/cleaned_phishing.csv\")\n",
    "\n",
    "# Load dataset (robust to common column names)\n",
    "df = pd.read_csv(RAW)\n",
    "print(\"Raw shape:\", df.shape)\n",
    "print(\"Columns:\", df.columns.tolist())\n",
    "\n",
    "# Identify text column\n",
    "text_col = None\n",
    "for c in ['text_combined','text','message','body','content']:\n",
    "    if c in df.columns:\n",
    "        text_col = c\n",
    "        break\n",
    "if text_col is None:\n",
    "    raise KeyError(\"Couldn't find a text column. Please ensure CSV has one of: text_combined, text, message, body, content\")\n",
    "print(\"Using text column:\", text_col)\n",
    "\n",
    "# If no label column, try to find a reasonable one, else create placeholder\n",
    "label_col = None\n",
    "for c in ['label','spam','class','target']:\n",
    "    if c in df.columns:\n",
    "        label_col = c\n",
    "        break\n",
    "if label_col is None:\n",
    "    print(\"No label column found; creating 'label' with default 1 (phishing) for all rows\")\n",
    "    df['label'] = 1\n",
    "    label_col = 'label'\n",
    "print(\"Using label column:\", label_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a129f80",
   "metadata": {},
   "source": [
    "### Cleaning Function and Apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bb5aabe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After cleaning — sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_combined</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hpl nom may 25 2001 see attached file hplno 52...</td>\n",
       "      <td>hpl nom may see attach file hplno xl hplno xl</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nom actual vols 24 th forwarded sabrae zajac h...</td>\n",
       "      <td>nom actual vol th forward sabra zajac hou ect ...</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>enron actuals march 30 april 1 201 estimated a...</td>\n",
       "      <td>enron actual march april estim actual march fl...</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hpl nom may 30 2001 see attached file hplno 53...</td>\n",
       "      <td>hpl nom may see attach file hplno xl hplno xl</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hpl nom june 1 2001 see attached file hplno 60...</td>\n",
       "      <td>hpl nom june see attach file hplno xl hplno xl</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       text_combined  \\\n",
       "0  hpl nom may 25 2001 see attached file hplno 52...   \n",
       "1  nom actual vols 24 th forwarded sabrae zajac h...   \n",
       "2  enron actuals march 30 april 1 201 estimated a...   \n",
       "3  hpl nom may 30 2001 see attached file hplno 53...   \n",
       "4  hpl nom june 1 2001 see attached file hplno 60...   \n",
       "\n",
       "                                          clean_text  word_count  \n",
       "0      hpl nom may see attach file hplno xl hplno xl          10  \n",
       "1  nom actual vol th forward sabra zajac hou ect ...         152  \n",
       "2  enron actual march april estim actual march fl...          17  \n",
       "3      hpl nom may see attach file hplno xl hplno xl          10  \n",
       "4     hpl nom june see attach file hplno xl hplno xl          10  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cleaning function (robust and documented)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text): return ''\n",
    "    s = str(text).lower()\n",
    "    # remove URLs and emails\n",
    "    s = re.sub(r'http\\S+|https\\S+|www\\.\\S+',' ', s)\n",
    "    s = re.sub(r'\\S+@\\S+',' ', s)\n",
    "    # remove HTML tags\n",
    "    s = re.sub(r'<.*?>',' ', s)\n",
    "    # remove non-letter characters\n",
    "    s = re.sub(r'[^a-z\\s]', ' ', s)\n",
    "    # collapse spaces\n",
    "    s = re.sub(r'\\s+', ' ', s).strip()\n",
    "    # tokenize, remove stopwords, stem\n",
    "    tokens = [stemmer.stem(w) for w in s.split() if w not in stop_words and len(w)>1]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply cleaning (this may take some time depending on dataset size)\n",
    "df['clean_text'] = df[text_col].apply(clean_text)\n",
    "df['word_count'] = df['clean_text'].apply(lambda s: len(s.split()))\n",
    "print(\"After cleaning — sample:\")\n",
    "display(df[[text_col, 'clean_text', 'word_count']].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb11d9e8",
   "metadata": {},
   "source": [
    "### Basic QC and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a172ebbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null clean_text rows: 0\n",
      "Min/Max word_count: 0 463175\n",
      "Label distribution:\n",
      "label\n",
      "1    42891\n",
      "0    39595\n",
      "Name: count, dtype: int64\n",
      "Saved cleaned data to ..\\data\\processed\\cleaned_phishing.csv\n"
     ]
    }
   ],
   "source": [
    "# Quick QC\n",
    "print(\"Null clean_text rows:\", df['clean_text'].isna().sum())\n",
    "print(\"Min/Max word_count:\", df['word_count'].min(), df['word_count'].max())\n",
    "print(\"Label distribution:\")\n",
    "print(df[label_col].value_counts(dropna=False))\n",
    "\n",
    "# Ensure processed dir exists and save\n",
    "OUT.parent.mkdir(parents=True, exist_ok=True)\n",
    "df.to_csv(OUT, index=False)\n",
    "print(\"Saved cleaned data to\", OUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938dde47",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- We stemmed words (Porter); if you prefer lemmatization for interpretability, replace PorterStemmer with WordNetLemmatizer (requires wordnet download).\n",
    "- `clean_text` removes URLs and email addresses so rules represent tokens (words) rather than raw links."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
